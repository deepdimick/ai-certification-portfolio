{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# LangChain RAG Search\n",
    "\n",
    "This notebook demonstrates various retrieval strategies for Retrieval-Augmented Generation (RAG) using LangChain and IBM Watsonx AI.\n",
    "\n",
    "## Overview\n",
    "- **LLM**: IBM Watsonx AI (Mistral-Small)\n",
    "- **Vector Store**: ChromaDB\n",
    "- **Embeddings**: IBM Slate-125M English Retriever\n",
    "\n",
    "## Retrieval Strategies Covered\n",
    "1. Vector Store-Backed Retrieval (similarity search, MMR, score threshold)\n",
    "2. Multi-Query Retriever\n",
    "3. Self-Querying Retriever\n",
    "4. Parent Document Retriever\n",
    "\n",
    "## Requirements\n",
    "- Python 3.9+\n",
    "- IBM Watsonx AI account with API credentials\n",
    "- Internet connection for downloading sample documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "install",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary Python packages\n",
    "!pip install \"ibm-watsonx-ai==1.1.2\" | tail -n 1\n",
    "!pip install \"langchain==0.2.1\" | tail -n 1\n",
    "!pip install \"langchain-ibm==0.1.11\" | tail -n 1\n",
    "!pip install \"langchain-community==0.2.1\" | tail -n 1\n",
    "!pip install \"chromadb==0.4.24\" | tail -n 1\n",
    "!pip install \"pypdf==4.3.1\" | tail -n 1\n",
    "!pip install 'posthog<6.0.0' | tail -n 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "warnings",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress warnings\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "llm-setup",
   "metadata": {},
   "source": [
    "## LLM Configuration\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "llm-header-orig",
   "metadata": {},
   "source": [
    "### Build retriever model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "llm-imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Use IBM Watsonx AI as the base LLM\n",
    "from ibm_watsonx_ai.foundation_models import ModelInference\n",
    "from ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams\n",
    "from ibm_watsonx_ai import Credentials\n",
    "from ibm_watsonx_ai.foundation_models.extensions.langchain import WatsonxLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "llm-function",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the llm function to initialize and return the IBM Watsonx AI LLM and its parameters\n",
    "def llm():\n",
    "    model_id = 'mistralai/mistral-small-3-1-24b-instruct-2503'\n",
    "    \n",
    "    parameters = {\n",
    "        GenParams.MAX_NEW_TOKENS: 256, \n",
    "        GenParams.TEMPERATURE: 0.4,\n",
    "    }\n",
    "    \n",
    "    credentials = {\n",
    "        \"url\": \"Input your IBM Watsonx AI URL\", # Replace with your actual IBM Watsonx AI URL\n",
    "        \"api_key\": \"Input your IBM Watsonx AI API Key\", # Replace with your actual API key\n",
    "    }\n",
    "    \n",
    "    project_id = 'your_project_id'  # Replace with your actual project ID\n",
    "    \n",
    "    model = ModelInference(\n",
    "        model_id=model_id,\n",
    "        params=parameters,\n",
    "        credentials=credentials,\n",
    "        project_id=project_id\n",
    "    )\n",
    "    \n",
    "    mixtral_llm = WatsonxLLM(model = model)\n",
    "    return mixtral_llm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "text-splitting",
   "metadata": {},
   "source": [
    "## Text Splitting Configuration\n",
    "\n",
    "Configure recursive character text splitter for document chunking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "text-splitter-imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "text-splitter-function",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the text_splitter function to split the input data into chunks\n",
    "def text_splitter(data, chunk_size, chunk_overlap):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len,\n",
    "    )\n",
    "    chunks = text_splitter.split_documents(data)\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "embedding-setup",
   "metadata": {},
   "source": [
    "## Embedding Model Setup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "embedding-header-orig",
   "metadata": {},
   "source": [
    "### Create the embedding model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "embedding-imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the embedding function to generate embeddings for the input data\n",
    "from ibm_watsonx_ai.metanames import EmbedTextParamsMetaNames\n",
    "from langchain_ibm import WatsonxEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "embedding-function",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the watsonx_embedding function to initialize and return the IBM Watsonx AI embedding model\n",
    "def watsonx_embedding():\n",
    "    embed_params = {\n",
    "        EmbedTextParamsMetaNames.TRUNCATE_INPUT_TOKENS: 3,\n",
    "        EmbedTextParamsMetaNames.RETURN_OPTIONS: {\"input_text\": True},\n",
    "    }\n",
    "    \n",
    "    watsonx_embedding = WatsonxEmbeddings(\n",
    "        model_id=\"ibm/slate-125m-english-rtrvr\",\n",
    "        url=\"https://us-south.ml.cloud.ibm.com\",\n",
    "        project_id=\"skills-network\",\n",
    "        params=embed_params,\n",
    "    )\n",
    "    return watsonx_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "retrievers",
   "metadata": {},
   "source": [
    "## Retrieval Strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vector-store-header-orig",
   "metadata": {},
   "source": [
    "### Initiate and use Different Retrievers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vector-backed-header-orig",
   "metadata": {},
   "source": [
    "#### Vector Store-Backed Retriever\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "download-text",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sample text for retriever from a URL\n",
    "!wget \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/MZ9z1lm-Ui3YBp3SYWLTAQ/companypolicies.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-text-imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import TextLoader to load text data\n",
    "from langchain_community.document_loaders import TextLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-text-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the text data from the file\n",
    "loader = TextLoader(\"companypolicies.txt\")\n",
    "txt_data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chunk-text",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunk the text data into 350-character chunks with 50-character overlap\n",
    "chunks_txt = text_splitter(txt_data, 350, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vectorstore-imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Chroma to create a vector store\n",
    "from langchain_community.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-vectorstore",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a vector store from the chunks using the IBM Watsonx AI embedding model and ChromaDB\n",
    "vectordb = Chroma.from_documents(chunks_txt, watsonx_embedding())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "similarity-header-orig",
   "metadata": {},
   "source": [
    "##### Simple similarity search\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "simple-retriever",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define as example query and create a retriever from the vector store; note that default is k=4 for the number of top chunks to retrieve\n",
    "query = \"email policy\"\n",
    "retriever = vectordb.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "simple-invoke",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invoke the retriever with the example query to retrieve relevant documents\n",
    "docs = retriever.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "simple-display",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the retrieved documents\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "simple-k2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now set retriever with a different number of top chunks to retrieve (e.g., k=2)\n",
    "retriever = vectordb.as_retriever(search_kwargs={\"k\": 2})\n",
    "docs = retriever.invoke(query)\n",
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mmr-header-orig",
   "metadata": {},
   "source": [
    "##### MMR search\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mmr-description-orig",
   "metadata": {},
   "source": [
    "MMR reduces redundancy while maintaining query relevance by balancing how relevant a document is to the query with how different it is from already-selected items. It's useful for RAG systems with limited context windows, selecting relevant but non-duplicate snippets. The main drawbacks are higher computational cost than simple similarity search and the need to tune a diversity parameter for your use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mmr-retriever",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set retriever to a an MMR (Maximal Marginal Relevance) search type\n",
    "retriever = vectordb.as_retriever(search_type=\"mmr\")\n",
    "docs = retriever.invoke(query)\n",
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "threshold-header-orig",
   "metadata": {},
   "source": [
    "##### Similarity score threshold retrieval\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "threshold-description-orig",
   "metadata": {},
   "source": [
    "Similarity score threshold retrieval returns all documents with similarity scores above a minimum threshold, very useful if you don't know the correct k value upfront. It retrieves only highly relevant documents based on a similarity score threshold, filtering out less relevant ones. The main benefit is control as it ensures only sufficiently relevant results surface. However, it may return zero results if nothing meets the threshold, and may require tuning of threshold values for your use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "threshold-retriever",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set retriever to use a similarity score threshold for retrieval\n",
    "retriever = vectordb.as_retriever(\n",
    "    search_type=\"similarity_score_threshold\", search_kwargs={\"score_threshold\": 0.4}\n",
    ")\n",
    "docs = retriever.invoke(query)\n",
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "multi-query-header-orig",
   "metadata": {},
   "source": [
    "#### Multi-Query Retriever\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "multi-query-description-orig",
   "metadata": {},
   "source": [
    "Generates multiple variations of the user query using an LLM, then searches with all variations and merges the results. It retrieves multiple sets of documents based on varied interpretations, which increases the likelihood of pinpointing relevant answers for vague or imprecisely formulated queries. Can improve recall by capturing documents from different phrasings and perspectives, however, it increases computational cost due to multiple LLM calls and vector searches and may also create potential noise from less relevant variations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-pdf-imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import PyPDFLoader to load PDF data\n",
    "from langchain_community.document_loaders import PyPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-pdf-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sample PDF for retriever from a URL\n",
    "loader = PyPDFLoader(\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/ioch1wsxkfqgfLLgmd-6Rw/langchain-paper.pdf\")\n",
    "pdf_data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chunk-pdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunk the PDF data into 500-character chunks with 20-character overlap\n",
    "chunks_pdf = text_splitter(pdf_data, 500, 20)\n",
    "\n",
    "## Create a vector store from the PDF chunks using the IBM Watsonx AI embedding model and ChromaDB\n",
    "# Get existing document\n",
    "ids = vectordb.get()[\"ids\"]\n",
    "# Clear existing embeddings from previous documents\n",
    "vectordb.delete(ids) \n",
    "# Create a new vector store with the PDF chunks\n",
    "vectordb = Chroma.from_documents(documents=chunks_pdf, embedding=watsonx_embedding())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "multi-query-retriever-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import MultiQueryRetriever to create a retriever that can handle multiple queries\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "# Define an example query and create a retriever from the vector store\n",
    "query = \"What does the paper say about langchain?\"\n",
    "\n",
    "retriever = MultiQueryRetriever.from_llm(\n",
    "    retriever=vectordb.as_retriever(), llm=llm()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "multi-query-logging",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set logging level to INFO for MultiQueryRetriever\n",
    "import logging\n",
    "\n",
    "logging.basicConfig()\n",
    "logging.getLogger(\"langchain.retrievers.multi_query\").setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "multi-query-invoke",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invoke the retriever with the example query to retrieve relevant documents\n",
    "docs = retriever.invoke(query)\n",
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "self-query-header-orig",
   "metadata": {},
   "source": [
    "#### Self-Querying Retriever\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "self-query-description-orig",
   "metadata": {},
   "source": [
    "Uses an LLM to write a structured query from natural language, allowing it to extract filters from user queries on document metadata and execute those filters alongside semantic similarity comparison.This can achieve higher precision but is dependent on the underlying LLM; if the model misinterprets ambiguous queries or metadata is poorly structured, filtering accuracy suffers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "self-query-imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries for self-querying\n",
    "from langchain_core.documents import Document\n",
    "from langchain.chains.query_constructor.base import AttributeInfo\n",
    "from langchain.retrievers.self_query.base import SelfQueryRetriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "self-query-docs",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a list of documents with metadata for self-querying\n",
    "docs = [\n",
    "    Document(\n",
    "        page_content=\"A master thief assembles a crew to pull off an impossible heist in a sprawling casino\",\n",
    "        metadata={\"year\": 2001, \"rating\": 8.4, \"genre\": \"heist\"},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"A rogue AI system threatens humanity while scientists race against time to shut it down\",\n",
    "        metadata={\"year\": 2015, \"director\": \"Alex Garland\", \"rating\": 7.9},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"A brilliant mathematician discovers hidden patterns in reality that no one else can perceive\",\n",
    "        metadata={\"year\": 1998, \"director\": \"Darren Aronofsky\", \"rating\": 8.5},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"A group of astronauts venture into deep space searching for signals of intelligent life\",\n",
    "        metadata={\"year\": 2014, \"director\": \"Denis Villeneuve\", \"rating\": 8.0},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Sentient robots develop consciousness and question their purpose in society\",\n",
    "        metadata={\"year\": 2004, \"genre\": \"science fiction\"},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"A lone wanderer navigates through an apocalyptic landscape seeking redemption\",\n",
    "        metadata={\n",
    "            \"year\": 1985,\n",
    "            \"director\": \"Ridley Scott\",\n",
    "            \"genre\": \"post-apocalyptic\",\n",
    "            \"rating\": 9.1,\n",
    "        },\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "self-query-metadata",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define metadata field information for self-querying\n",
    "metadata_field_info = [\n",
    "    AttributeInfo(\n",
    "        name=\"genre\",\n",
    "        description=\"The category or classification of the film. Select from ['science fiction', 'comedy', 'drama', 'thriller', 'romance', 'action', 'animated']\",\n",
    "        type=\"string\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"year\",\n",
    "        description=\"The release date of the film (expressed as a calendar year)\",\n",
    "        type=\"integer\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"director\",\n",
    "        description=\"The filmmaker responsible for directing the production\",\n",
    "        type=\"string\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"rating\", description=\"A numerical score between 1 and 10 indicating quality\", type=\"float\"\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "self-query-vectordb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a vector store from the documents using the IBM Watsonx AI embedding model and ChromaDB\n",
    "vectordb = Chroma.from_documents(docs, watsonx_embedding())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "self-query-retriever",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initialize the self-querying retriever with the LLM, vector store, document content description, and metadata field information\n",
    "document_content_description = \"Brief summary of a movie.\"\n",
    "\n",
    "retriever = SelfQueryRetriever.from_llm(\n",
    "    llm(),\n",
    "    vectordb,\n",
    "    document_content_description,\n",
    "    metadata_field_info,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "self-query-test1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the self-querying retriever with a query\n",
    "retriever.invoke(\"I want to watch a movie rated higher than 7\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "self-query-test2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the self-querying retriever with a query that includes metadata\n",
    "retriever.invoke(\"Find films directed by Denis Villeneuve in the science fiction category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "self-query-test3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This example specifies a composite filter\n",
    "retriever.invoke(\"Show me an action film with an excellent rating (above 8.0)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "parent-doc-header-orig",
   "metadata": {},
   "source": [
    "#### Parent Document Retriever\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "parent-doc-description-orig",
   "metadata": {},
   "source": [
    "Chunks large documents into smaller sub-documents, queries the smaller chunks for precision, then returns the full parent document to the LLM. This approach ensures the system benefits from accurate child document embeddings for precise retrieval while providing the LLM with the broader parent document context, resulting in more extensive and detailed answers. The downside is increased storage overhead (maintaining both parent and child documents) and potentially passing too much context to the LLM, which can introduce noise or dilute signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parent-doc-imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries for parent document retriever\n",
    "from langchain.retrievers import ParentDocumentRetriever\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain.storage import InMemoryStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parent-doc-splitters",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set two splitters. One is with big chunk size (parent) and one is with small chunk size (child)\n",
    "parent_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=20, separator='\\n')\n",
    "child_splitter = CharacterTextSplitter(chunk_size=200, chunk_overlap=20, separator='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parent-doc-retriever",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a vector store for the parent documents using the IBM Watsonx AI embedding model and ChromaDB\n",
    "vectordb = Chroma(\n",
    "    collection_name=\"split_parents\", embedding_function=watsonx_embedding()\n",
    ")\n",
    "\n",
    "# The storage layer for the parent documents\n",
    "store = InMemoryStore()\n",
    "\n",
    "# Initialize the parent document retriever with the vector store, document store, and splitters\n",
    "retriever = ParentDocumentRetriever(\n",
    "    vectorstore=vectordb,\n",
    "    docstore=store,\n",
    "    child_splitter=child_splitter,\n",
    "    parent_splitter=parent_splitter,\n",
    ")\n",
    "\n",
    "# Add the text data to the parent document retriever\n",
    "retriever.add_documents(txt_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parent-doc-verify-parents",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the number of parent documents\n",
    "len(list(store.yield_keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parent-doc-verify-children",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the number of child documents\n",
    "sub_docs = vectordb.similarity_search(\"smoking policy\")\n",
    "print(sub_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parent-doc-invoke",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve and verify documents using the parent document retriever\n",
    "retrieved_docs = retriever.invoke(\"smoking policy\")\n",
    "print(retrieved_docs[0].page_content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "prev_pub_hash": "c95e10fbc5e657b964f63cd018cbc353aa8bce320e08dc521791f1f33a57b510"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
